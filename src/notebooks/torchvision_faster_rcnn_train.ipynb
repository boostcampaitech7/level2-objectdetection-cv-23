{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = '../../dataset/train.json'\n",
    "coco = COCO(annotation)\n",
    "image_id = coco.getImgIds(imgIds=0)\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "ann_ids = coco.getAnnIds(imgIds=image_info['id'])\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "labels = np.array([x['category_id']+1 for x in anns]) \n",
    "labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxex (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        # torchvision faster_rcnn은 label=0을 background로 취급\n",
    "        # class_id를 1~10으로 수정 \n",
    "        labels = np.array([x['category_id']+1 for x in anns]) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "                                \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(1024, 1024),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(num_epochs, train_data_loader, optimizer, model, device):\n",
    "    best_loss = 1000\n",
    "    total_loss_hist = Averager()\n",
    "    cls_loss_hist = Averager()\n",
    "    box_loss_hist = Averager()\n",
    "    rpn_cls_loss_hist = Averager()\n",
    "    rpn_box_loss_hist = Averager()\n",
    "    \n",
    "    checkpoint_dir = './checkpoints'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_hist.reset()\n",
    "        cls_loss_hist.reset()\n",
    "        box_loss_hist.reset()\n",
    "        rpn_cls_loss_hist.reset()\n",
    "        rpn_box_loss_hist.reset()\n",
    "\n",
    "        for images, targets, image_ids in tqdm(train_data_loader):\n",
    "\n",
    "            # gpu 계산을 위해 image.to(device)\n",
    "            images = list(image.float().to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # calculate loss\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            total_loss_hist.send(loss_value)\n",
    "            \n",
    "            cls_loss_hist.send(loss_dict['loss_classifier'].item())\n",
    "            box_loss_hist.send(loss_dict['loss_box_reg'].item())\n",
    "            rpn_cls_loss_hist.send(loss_dict['loss_objectness'].item())\n",
    "            rpn_box_loss_hist.send(loss_dict['loss_rpn_box_reg'].item())\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        epoch_loss = total_loss_hist.value\n",
    "        loss_cls = cls_loss_hist.value\n",
    "        loss_box_reg = box_loss_hist.value\n",
    "        loss_rpn_cls = rpn_cls_loss_hist.value\n",
    "        loss_rpn_loc = rpn_box_loss_hist.value\n",
    "        \n",
    "        print(f\"Epoch #{epoch+1} loss: {epoch_loss}\")\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"total_loss\": epoch_loss,\n",
    "            \"loss_cls\": loss_cls,\n",
    "            \"loss_box_reg\" : loss_box_reg,\n",
    "            \"loss_rpn_cls\" : loss_rpn_cls,\n",
    "            \"loss_rpn_loc\" : loss_rpn_loc,\n",
    "            }, step=epoch)\n",
    "        \n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            \n",
    "            save_path = os.path.join(checkpoint_dir, f'faster_rcnn_torchvision_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "            # MLflow 모델 저장\n",
    "            mlflow.pytorch.log_model(model, f'faster_rcnn_torchvision_{epoch+1}.pth')\n",
    "            mlflow.log_artifact(save_path)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # mlflow tracking URI 설정 및 experiments 설정 \n",
    "    mlflow.set_tracking_uri(\"http://10.28.224.171:30280\")\n",
    "    experiment_name = \"Faster_RCNN_COCO\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    try:\n",
    "        with mlflow.start_run():\n",
    "            # 하이퍼파라미터 설정\n",
    "            num_epochs = 5\n",
    "            batch_size = 16\n",
    "            learning_rate = 0.005\n",
    "            momentum = 0.9\n",
    "            weight_decay = 0.0005\n",
    "        \n",
    "            # 데이터셋 불러오기\n",
    "            annotation = '/data/ephemeral/home/workspace/dataset/train.json' # annotation 경로\n",
    "            data_dir = '/data/ephemeral/home/workspace/dataset' # data_dir 경로\n",
    "            train_dataset = CustomDataset(annotation, data_dir, get_train_transform()) \n",
    "            train_data_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "            \n",
    "            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
    "            # torchvision model 불러오기\n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "            num_classes = 11 # class 개수= 10 + background\n",
    "            # get number of input features for the classifier\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "            model.to(device)\n",
    "            \n",
    "            # optimizer \n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        \n",
    "            # ML flow에 하이퍼 파라미터 로깅\n",
    "            mlflow.log_params({\n",
    "                    \"num_epochs\": num_epochs,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"momentum\": momentum,\n",
    "                    \"weight_decay\": weight_decay\n",
    "            })\n",
    "            \n",
    "            # training\n",
    "            trained_model = train_fn(num_epochs, train_data_loader, optimizer, model, device)\n",
    "            \n",
    "            # 최종 결과 로깅\n",
    "            final_dir = './final_models'\n",
    "            if not os.path.exists(final_dir):\n",
    "                os.makedirs(final_dir)\n",
    "            final_model_path = os.path.join(final_dir, \"final_model.pth\")\n",
    "            torch.save(trained_model.state_dict(), final_model_path)\n",
    "            mlflow.log_artifact(final_model_path)\n",
    "            \n",
    "            # model signature \n",
    "            trained_model.eval()\n",
    "            sample_input = next(iter(train_data_loader))[0][0].unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                sample_output = trained_model(sample_input)\n",
    "            input_sample = sample_input.cpu().numpy()\n",
    "            output_sample = {k: v.cpu().numpy() for k, v in sample_output[0].items()}\n",
    "            signature = infer_signature(input_sample, output_sample)\n",
    "            \n",
    "            # 최종 모델 로깅 \n",
    "            mlflow.pytorch.log_model(trained_model, \"final_model\", signature=signature)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occured: {e}\")\n",
    "        mlflow.log_param(\"error\", str(e))\n",
    "    finally:\n",
    "        mlflow.end_run()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
